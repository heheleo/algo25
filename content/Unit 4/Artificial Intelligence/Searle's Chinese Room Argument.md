Proposed by philosopher John Searle in 1980, the argument holds that a computer executing a program cannot have a mind, understanding or consciousness, regardless of how intelligently or human-like the program may make the computer behave.[^1]
## Thought experiment
1. Imagine a person who does not understand Chinese isolated in a room with a book containing detailed instructions for manipulating Chinese.
2. When Chinese text is passed into the room, the person follows the book instructions to produce an appropriate response to fluent Chinese speakers outside the room.
3. According to Searle, the person is simply following syntactic rules without semantic comprehension.
4. When computers execute programs, they are similarly just applying syntactic rules without understanding or thinking.
## Strong AI
Searle identifies [[Weak and Strong AI|Strong AI]] as: the appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have mind.

The definition depends on the distinction between simulating a mind and actually having one. Searle writes that "according to Strong AI, the correct simulation really is a mind. According to Weak AI, the correct simulation is a model of the mind.".
## Responses Against
- Systems reply: while the person in the room may not understand Chinese, the system as a whole (including the person, the rule book, the symbols, and the room itself) does. The output of the room, when viewed as a single entity, demonstrates understanding
- Robot reply: a robot, with its ability to interact with the physical world, could potentially achieve genuine understanding, unlike the person in the room who is isolated
- Brain simulator reply: if the computer's program simulated the actual neural firings of a Chinese speaker's brain, then the computer would understand Chinese, just as a native speaker does
## Responses For
Searle has responded to these arguments, primarily by reiterating that the person in the room, even when following all the rules and producing outputs that seem intelligent, still does not understand Chinese. He maintains that the system's behaviour is purely syntactic, lacking any semantic understanding. Searle's argument hinges on the distinction between syntax (manipulating symbols) and semantics (understanding meaning).

[^1]: https://en.wikipedia.org/wiki/Chinese_room
